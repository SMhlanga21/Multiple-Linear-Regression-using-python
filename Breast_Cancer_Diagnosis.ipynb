{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Breast Cancer Diagnosis Using Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "README:\n",
    "    \n",
    "    This project can be run in pycharm, but for better visual presentation it is ideal to run     it in Jupyter. The only thing needed is the dataset, which has to be in the same folder       as the .ipynb file to be imported without problems.\n",
    "    \n",
    "    The dataset can be downloaded from Kaggle. I have provided the link directly to the           dataset:\n",
    "    https://www.kaggle.com/uciml/breast-cancer-wisconsin-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is importing the packages I will need. I do this in the cell below, where I also import the dataset, which is saved in the same folder as this notebook. In addition, I split the data into dependent and independent variables. \n",
    "The dependent variable is Diagnosis, which is located in index 1 of the dataset. The rest of the columns are independent variables.\n",
    "\n",
    "I created the df and assigned it to the same dataset except utilizing pandas dataframe. I define that so I can use it for the independent \n",
    "variables. I would have preferred to use pandas for the dependet variables as well but it does not work well with the non numeric values in that variable. Therefore, I sliced it straight from the dataset, which will allow me to convert it into zeros and ones below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "      ...       texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0     ...               17.33           184.60      2019.0            0.1622   \n",
       "1     ...               23.41           158.80      1956.0            0.1238   \n",
       "2     ...               25.53           152.50      1709.0            0.1444   \n",
       "3     ...               26.50            98.87       567.7            0.2098   \n",
       "4     ...               16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing the necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import DataFrame \n",
    "\n",
    "#Loading the dataset\n",
    "dataset = pd.read_csv('data.csv')\n",
    "\n",
    "df = DataFrame(dataset,columns=['ID','Diagnosis','radius_mean','texture_mean','perimeter_mean','area_mean','smoothness_mean','compactness_mean','concavity_mean','concave points_mean','symmetry_mean','fractal_dimension_mean','radius_se','texture_se','perimeter_se','area_se','smoothness_se','compactness_se','concavity_se','concave points_se','symmetry_se','fractal_dimension_se','radius_worst','texture_worst','perimeter_worst','area_worst','smoothness_worst','compactness_worst','concavity_worst','concave points_worst','symmetry_worst','fractal_dimension_worst'])\n",
    "\n",
    "\n",
    "x = dataset.iloc[:, 2:-1]\n",
    "y = dataset.iloc[:, 1:2].values\n",
    "\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having separated the dataset into x and y values, the first thing I need to do is convert the y values from M and B to 1 and 0. \n",
    "To do this I use the LabelEncoder class from sklearn. This transforms the Ms into ones and B into zeros and that is assigned to \n",
    "a new y variable which now carries numerical values that can be used in regression.\n",
    "I also in the same cell split the data into the train and test set so that I can test my regression model once I have trained it.\n",
    "From this cell I emerge with x_train, y_train, s_test and y_test, which I derive from x and y using train_test_split class from asklearn model selection\n",
    "I also utilized another sklearn class StandardScaler to normalize the features for better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "y = labelencoder.fit_transform(y)\n",
    "\n",
    "#splitting the data into training and testing set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "#normalizing data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc_x = StandardScaler()\n",
    "x_train = sc_x.fit_transform(x_train)\n",
    "x_test = sc_x.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below I calculate the Variance Inflation Factor for each of the indepedent variables to determine multicollinearity.\n",
    "I used statsmodels to do but used for loop to iterate through all the features in x because it has multiple features. \n",
    "The majority of the feature have very high VIFs, which is a sign that they explain the same thing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      VIF Factor                 features\n",
      "0   63306.172036              radius_mean\n",
      "1     251.047108             texture_mean\n",
      "2   58123.586079           perimeter_mean\n",
      "3    1287.262339                area_mean\n",
      "4     393.398166          smoothness_mean\n",
      "5     200.980354         compactness_mean\n",
      "6     157.855046           concavity_mean\n",
      "7     154.241268      concave points_mean\n",
      "8     184.426558            symmetry_mean\n",
      "9     629.679874   fractal_dimension_mean\n",
      "10    236.665738                radius_se\n",
      "11     24.675367               texture_se\n",
      "12    211.396334             perimeter_se\n",
      "13     72.466468                  area_se\n",
      "14     26.170243            smoothness_se\n",
      "15     44.919651           compactness_se\n",
      "16     33.244099             concavity_se\n",
      "17     53.698656        concave points_se\n",
      "18     37.176452              symmetry_se\n",
      "19     27.532631     fractal_dimension_se\n",
      "20   9674.742602             radius_worst\n",
      "21    343.004387            texture_worst\n",
      "22   4487.781270          perimeter_worst\n",
      "23   1138.759252               area_worst\n",
      "24    375.597155         smoothness_worst\n",
      "25    132.884276        compactness_worst\n",
      "26     86.310362          concavity_worst\n",
      "27    148.673180     concave points_worst\n",
      "28    218.919805           symmetry_worst\n",
      "29    423.396723  fractal_dimension_worst\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(x.values, i) for i in range(x.shape[1])]\n",
    "vif[\"features\"] = x.columns\n",
    "print(vif)\n",
    "#vif.round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to run multiple linear regression. One way to do this would be to use python to do all the calculations\n",
    "for regression. Another is to import the LinearRegression class from sklearn. While the former method would make it easier to \n",
    "manipulate results and how they loook in output, it would take long and is somewhat redundant as sklearn already has a class\n",
    "that does the same thing. Therefore, for the sake of not reiventing the wheel, I opted for sklearn again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training multiple linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regr = LinearRegression()\n",
    "regr.fit(x_train,y_train)\n",
    "\n",
    "#prediction on the dataset\n",
    "\n",
    "y_pred= regr.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I will run a logistic regression model with the same dataset. While I set out to run a logistic regression model, I \n",
    "though it would be a good idea to try another model and see how it works. The biggest and most important step in any data\n",
    "analysis is dataset preprocessing, making sure the data is free of missing values, is normalized if needed as well as recoding \n",
    "any values that need to be recoded, and this is where python is a major asset and where I intend to strengthen my skills. \n",
    "Once that has been done, as I have above, the kind of analysis or model that is being run on the dataset does not take long and\n",
    "one could easily run multiple models to find one that is more suitable for the dataset and the problem at hand. However, since the\n",
    "aim of my project is to show the steps taken to do this in python, I am only focusing on multiple linear regression and have included \n",
    "logistic regression to underline the point that multiple different models can be run as long as the dataset meets the specific\n",
    "assumptions of the model you intend to run. The model score for logistic is quite high at 96.49%, which is somewhat expected given that \n",
    "the model has been given 30 features, even though we already know from the VIF that if further analysis was to be done, most of the variables would need\n",
    "to be removed as they do not add enough value while keeping the complexity high. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit score:  0.9649122807017544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SIlVERDOLLAR/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Scikit Logistic Regression\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "scikit_log_reg = LogisticRegression()\n",
    "scikit_log_reg.fit(x_train,y_train)\n",
    "\n",
    "scikit_score = scikit_log_reg.score(x_test,y_test)\n",
    "print ('Scikit score: ', scikit_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I run statsmodel to get the results of my regression model in order to do the kind of analysis that looks at how useful\n",
    "the model is, what variables can be taken seriously and which can be removed to maximize model value while minimizing complexity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.853\n",
      "Model:                            OLS   Adj. R-squared:                  0.844\n",
      "Method:                 Least Squares   F-statistic:                     103.9\n",
      "Date:                Thu, 16 May 2019   Prob (F-statistic):          1.78e-202\n",
      "Time:                        23:56:27   Log-Likelihood:                 18.089\n",
      "No. Observations:                 569   AIC:                             23.82\n",
      "Df Residuals:                     539   BIC:                             154.1\n",
      "Df Model:                          30                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===========================================================================================\n",
      "                              coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------\n",
      "radius_mean                -0.3531      0.174     -2.024      0.043      -0.696      -0.010\n",
      "texture_mean                0.0033      0.008      0.403      0.687      -0.013       0.019\n",
      "perimeter_mean              0.0239      0.026      0.932      0.352      -0.026       0.074\n",
      "area_mean                   0.0013      0.000      2.764      0.006       0.000       0.002\n",
      "smoothness_mean             0.2626      2.056      0.128      0.898      -3.777       4.302\n",
      "compactness_mean           -1.4791      1.224     -1.208      0.227      -3.884       0.926\n",
      "concavity_mean              1.0316      1.063      0.970      0.332      -1.057       3.121\n",
      "concave points_mean         3.0280      2.009      1.507      0.132      -0.918       6.974\n",
      "symmetry_mean              -0.4348      0.748     -0.581      0.561      -1.905       1.035\n",
      "fractal_dimension_mean    -18.6170      4.009     -4.644      0.000     -26.493     -10.741\n",
      "radius_se                   0.3957      0.316      1.251      0.212      -0.226       1.017\n",
      "texture_se                 -0.0008      0.038     -0.022      0.982      -0.075       0.073\n",
      "perimeter_se               -0.0122      0.042     -0.292      0.770      -0.094       0.070\n",
      "area_se                    -0.0017      0.001     -1.228      0.220      -0.005       0.001\n",
      "smoothness_se              14.5322      6.749      2.153      0.032       1.276      27.789\n",
      "compactness_se             -1.8382      2.173     -0.846      0.398      -6.108       2.431\n",
      "concavity_se               -3.5134      1.326     -2.649      0.008      -6.118      -0.908\n",
      "concave points_se          10.7609      5.559      1.936      0.053      -0.158      21.680\n",
      "symmetry_se                 1.4318      2.780      0.515      0.607      -4.030       6.894\n",
      "fractal_dimension_se        7.8849     11.454      0.688      0.492     -14.616      30.386\n",
      "radius_worst                0.2336      0.059      3.993      0.000       0.119       0.349\n",
      "texture_worst               0.0063      0.007      0.888      0.375      -0.008       0.020\n",
      "perimeter_worst            -0.0054      0.006     -0.900      0.368      -0.017       0.006\n",
      "area_worst                 -0.0011      0.000     -3.463      0.001      -0.002      -0.000\n",
      "smoothness_worst           -0.0697      1.457     -0.048      0.962      -2.931       2.792\n",
      "compactness_worst          -0.0775      0.389     -0.199      0.842      -0.842       0.687\n",
      "concavity_worst             0.4385      0.274      1.603      0.110      -0.099       0.976\n",
      "concave points_worst        0.5394      0.932      0.579      0.563      -1.291       2.370\n",
      "symmetry_worst              0.6380      0.504      1.267      0.206      -0.351       1.627\n",
      "fractal_dimension_worst     5.3423      2.420      2.208      0.028       0.589      10.095\n",
      "==============================================================================\n",
      "Omnibus:                       34.960   Durbin-Watson:                   1.837\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               39.782\n",
      "Skew:                           0.617   Prob(JB):                     2.30e-09\n",
      "Kurtosis:                       3.396   Cond. No.                     1.49e+06\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.49e+06. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "#model performance\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "regr_OLS= sm.OLS(endog= y, exog= x).fit()\n",
    "mod_sum = regr_OLS.summary()\n",
    "print(mod_sum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above show all the features and their p-values, among other things. Most of them have high p-values and therefore will be removed \n",
    "below when I run the model again, this time time with only those features with p-values less than 0.05, as is customary in statistical analysis.\n",
    "The other important statistics shown above are Adjusted R squared and Durbin-Watson statistic. Adjusted R-squared is very, which happens when there number\n",
    "of variables is high, therefore we will not read too much into it. The Durbin-Watson statistic is 1.837, which means there is very little autocorrelation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    VIF Factor                 features\n",
      "0   207.805209   fractal_dimension_mean\n",
      "1  2998.126668              radius_mean\n",
      "2   579.701192                area_mean\n",
      "3     9.507260            smoothness_se\n",
      "4  2781.321159             radius_worst\n",
      "5   529.860170               area_worst\n",
      "6    74.256058  fractal_dimension_worst\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.811\n",
      "Model:                            OLS   Adj. R-squared:                  0.808\n",
      "Method:                 Least Squares   F-statistic:                     343.9\n",
      "Date:                Fri, 17 May 2019   Prob (F-statistic):          1.79e-198\n",
      "Time:                        00:01:30   Log-Likelihood:                -52.948\n",
      "No. Observations:                 569   AIC:                             119.9\n",
      "Df Residuals:                     562   BIC:                             150.3\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===========================================================================================\n",
      "                              coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------\n",
      "fractal_dimension_mean    -20.2232      2.555     -7.914      0.000     -25.243     -15.204\n",
      "radius_mean                -0.4046      0.042     -9.605      0.000      -0.487      -0.322\n",
      "area_mean                   0.0032      0.000      8.924      0.000       0.003       0.004\n",
      "smoothness_se              23.0322      4.513      5.103      0.000      14.168      31.897\n",
      "radius_worst                0.3794      0.035     10.898      0.000       0.311       0.448\n",
      "area_worst                 -0.0022      0.000     -8.853      0.000      -0.003      -0.002\n",
      "fractal_dimension_worst     9.8286      1.124      8.742      0.000       7.620      12.037\n",
      "==============================================================================\n",
      "Omnibus:                       50.954   Durbin-Watson:                   1.902\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               64.435\n",
      "Skew:                           0.728   Prob(JB):                     1.02e-14\n",
      "Kurtosis:                       3.771   Cond. No.                     5.51e+05\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 5.51e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "[ 0.56852587  0.10369417  0.08861622  0.30040167 -0.16391851 -0.02784928\n",
      " -0.02886042 -0.07232909 -0.04417258  0.06920224  0.50124734  0.25577682\n",
      " -0.25737093  0.30838785  0.38000791  0.67413294 -0.10273295  0.9171733\n",
      "  0.77837848  0.97266206  0.95604941  0.68683498  0.24215738  0.08621819\n",
      "  0.80441822  0.17617829  0.03421582  0.54321493  0.16673577  1.12143294\n",
      "  0.08850167  0.97709231  0.26912897  0.62115805 -0.19596866  0.83699633\n",
      "  0.28625254  0.7573935   0.33696804  0.84234374  0.39610581 -0.1595852\n",
      "  0.62929848 -0.14231872  0.5870185   1.02168382 -0.3203205   0.28147169\n",
      " -0.06636027  0.81526068  0.65887907  0.36763224  0.82847979  0.06211197\n",
      "  0.06003677  0.11289858 -0.16806766  0.04553777  0.00759481  1.24511943\n",
      "  0.71402391  0.76186543 -0.11924571  0.05315018  1.05157559  0.11916416\n",
      "  1.3044668   1.15121577  0.78433063 -0.06217223  0.40975307  1.0729169\n",
      "  0.01348685  0.3211567   1.01771297  0.11010268 -0.15065196  0.21714038\n",
      "  0.08369793  0.08286218  0.57604785  1.04525926  1.05836607 -0.10642469\n",
      "  0.69054668  0.08646325  0.08799464 -0.345647    0.81597329  1.54946794\n",
      " -0.21179073  0.61846967  0.44190217  0.8382788   0.11049535 -0.09471241\n",
      "  1.17972551  0.40552254  0.13633261  0.03976712  0.02953769 -0.07161577\n",
      "  0.14510085  0.30136397  0.99598262  0.00498418  0.99155032  0.29531542\n",
      "  0.40018877  0.95374889  0.28380587  0.82903492  0.78577007  0.33430326]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/SIlVERDOLLAR/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "x = df[['fractal_dimension_mean','radius_mean','area_mean','smoothness_se','radius_worst','area_worst','fractal_dimension_worst']]\n",
    "y = dataset.iloc[:, 1:2].values\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "y = labelencoder.fit_transform(y)\n",
    "\n",
    "#splitting the data into training and testing set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "#normalizing data\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc_x = StandardScaler()\n",
    "x_train = sc_x.fit_transform(x_train)\n",
    "x_test = sc_x.transform(x_test)\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(x.values, i) for i in range(x.shape[1])]\n",
    "vif[\"features\"] = x.columns\n",
    "print(vif)\n",
    "#vif.round(1)\n",
    "\n",
    "#training multiple linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regr1 = LinearRegression()\n",
    "regr1.fit(x_train,y_train)\n",
    "\n",
    "#prediction on the dataset\n",
    "\n",
    "y_pred= regr1.predict(x_test)\n",
    "\n",
    "#model performance\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "regr1_OLS= sm.OLS(endog= y, exog= x).fit()\n",
    "mod_sum = regr1_OLS.summary()\n",
    "print(mod_sum)\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above only show those features whose p-values show them to be of statistical significance.\n",
    "As such, these results can be taken more seriously and the model can now be used to predict diagnosis. The adjusted R-squared went down to 0.808,\n",
    "which is not a big fall given that now we are using 7 features as opposed to the 30 we began with. Durbin-Watson statistic also\n",
    "increased and get closer to 2, meaning reduction of features helped us get rid of more autocorrelation. \n",
    "\n",
    "Below I print the prediction results from the model and the test results. The results are encouraging. Those that are ones in y_test are above 0.5 in \n",
    "y_pred, in many cases very close to 1 and those that are zeros in the actual dataset are very close to \n",
    "zero in predicted values, which means the model does a good job of predicting diagnosis. \n",
    "Using the for loop, I convert the values greater than 0.5 to 'B' and all other values to to 'M', to basically get the Diagnosis as it would show in the original Dataset. For test purposes, in the same print statement I print the corresponding value from the test data, where 0 represents 'B' and 1 represents 'M', to compare my predicted values next to the original values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B\n",
      "M\n",
      "M\n",
      "M\n",
      "M\n",
      "M\n",
      "M\n",
      "M\n",
      "M\n",
      "M\n",
      "B\n",
      "M\n",
      "M\n",
      "M\n",
      "M\n",
      "B\n",
      "M\n",
      "B\n",
      "B\n",
      "B\n",
      "B\n",
      "B\n",
      "M\n",
      "M\n",
      "B\n",
      "M\n",
      "M\n",
      "B\n",
      "M\n",
      "B\n",
      "M\n",
      "B\n",
      "M\n",
      "B\n",
      "M\n",
      "B\n",
      "M\n",
      "B\n",
      "M\n",
      "B\n",
      "M\n",
      "M\n",
      "B\n",
      "M\n",
      "B\n",
      "B\n",
      "M\n",
      "M\n",
      "M\n",
      "B\n",
      "B\n",
      "M\n",
      "B\n",
      "M\n",
      "M\n",
      "M\n",
      "M\n",
      "M\n",
      "M\n",
      "B\n",
      "B\n",
      "B\n",
      "M\n",
      "M\n",
      "B\n",
      "M\n",
      "B\n",
      "B\n",
      "B\n",
      "M\n",
      "M\n",
      "B\n",
      "M\n",
      "M\n",
      "B\n",
      "M\n",
      "M\n",
      "M\n",
      "M\n",
      "M\n",
      "B\n",
      "B\n",
      "B\n",
      "M\n",
      "B\n",
      "M\n",
      "M\n",
      "M\n",
      "B\n",
      "B\n",
      "M\n",
      "B\n",
      "M\n",
      "B\n",
      "M\n",
      "M\n",
      "B\n",
      "M\n",
      "M\n",
      "M\n",
      "M\n",
      "M\n",
      "M\n",
      "M\n",
      "B\n",
      "M\n",
      "B\n",
      "M\n",
      "M\n",
      "B\n",
      "M\n",
      "B\n",
      "B\n",
      "M\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0\n",
      " 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 1\n",
      " 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 0\n",
      " 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "#print('Predicted values of Diagnosis: ', y_pred)\n",
    "#print('The actual results: ', y_test)\n",
    "\n",
    "ytest = y_pred\n",
    "\n",
    "for num in (ytest):\n",
    "    if num > 0.50:\n",
    "        print('B')\n",
    "    else:\n",
    "        print('M')\n",
    "\n",
    "print(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
